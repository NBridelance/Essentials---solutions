{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Data Lake to Data Warehouse - ETL \n",
    "\n",
    "Creating robust pipeline that let your data flow seamlessly has become one of the most important part of a well-made infrastructure. These pipelines are called ETL processes. In this course, we will cover the basics of an ETL process and build some using Google Big Query ðŸª„\n",
    "\n",
    "## What you'll learn in this course ðŸ§ðŸ§\n",
    "\n",
    "* What is an ETL \n",
    "* What is Google Big Query\n",
    "* Build your first ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an ETL? \n",
    "\n",
    "ETL stands for **E**tract **T**ransform **L**oad. As barbaric as it sounds, it is simply the process of extracting data from your Data Lake to your Data Warehouse. \n",
    "\n",
    "You can build ETL using SQL, Python or even using No code solutions like cloud services.\n",
    "\n",
    "## What is Google Big Query ðŸ¥Š\n",
    "\n",
    "[Big Query](https://cloud.google.com/bigquery) is Google's cloud Data Warehouse. A lot of companies are using it these days as it as easy to use, reliable and relatively cheap. \n",
    "\n",
    "With Big Query, you will be able to: \n",
    "\n",
    "* Build simple ETLs \n",
    "* Query your data using SQL \n",
    "\n",
    "Let's first open Big Query by going on [Google Cloud Console](https://cloud.google.com) and search for *Big Query*\n",
    "\n",
    "![crack](https://essentials-assets.s3.eu-west-3.amazonaws.com/M02-SQL/Introduction_to_SQL_and_cloud_computing/big_query_search.png)\n",
    "\n",
    "On the left side of the screen, you will see the name of your project (in the above picture *fresh-desk-324610*). This is where your data is going to live. \n",
    "\n",
    "For the moment, it is empty ðŸ•³ï¸ as it should be but we will learn how to add data in the section of this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your first ETL ðŸ“©\n",
    "\n",
    "There are two ways you can transfer data to Big Query:\n",
    "\n",
    "1. From a Data Lake using *Data Transfer* service \n",
    "2. From a Cloud SQL database using *Big Query* built-in functionnalities\n",
    "\n",
    "Before diving into each option, let's prepare Big Query. First we will need to create a database:\n",
    "\n",
    "* **Create Dataset**\n",
    "\n",
    "![crack](https://essentials-assets.s3.eu-west-3.amazonaws.com/M02-SQL/Introduction_to_SQL_and_cloud_computing/create_dataset_bq.gif)\n",
    "\n",
    "We are all set, let's populate our tables with data. ðŸ’ª\n",
    "\n",
    "\n",
    "ðŸ‘‹ As a final note, **make sure you have some data within your Data Lake** (i.e Google Cloud Storage)\n",
    "\n",
    "\n",
    "### From Data Lake using Data Transfer ðŸ’¦\n",
    "\n",
    "\n",
    "Now that we have Big Query ready to receive data, let's add Data. There are two ways of doing it depending on how often you need to transfer data.  \n",
    "\n",
    "\n",
    "#### If you need to transfer data once \n",
    "\n",
    "If you only need to transfer your data from your Data Lake to Big Query once, it's relatively simple. You only need to create a table from your dataset and specify where to get the data: \n",
    "\n",
    "![crack](https://essentials-assets.s3.eu-west-3.amazonaws.com/M02-SQL/Introduction_to_SQL_and_cloud_computing/create_table_from_datalake.gif)\n",
    "\n",
    "Google will automaticall detect the type of file you want to import to Big Query. ðŸ˜® \n",
    "\n",
    "ðŸ‘‹ Select `auto detect` to automatically detect your files schema. It will save you some time otherwise you will need to specify each column of your table manually ðŸ˜°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you need to transfer data periodically \n",
    "\n",
    "If you need to periodically import data from your data lake to Big Query, you will need to specify it using *Data Transfer*. Here is how to do it: \n",
    "\n",
    "![crack](https://essentials-assets.s3.eu-west-3.amazonaws.com/M02-SQL/Introduction_to_SQL_and_cloud_computing/create_a_data_transfer.gif)\n",
    "\n",
    "âš ï¸ You need to already have a table created with the right schema to use *Data Transfer* âš ï¸ Therefore best practice is to first to a manual transfer so that you don't have to waste time building your schema and then create a periodical data transfer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data directly from cloud SQL â˜ï¸\n",
    "\n",
    "When you are not dealing with CSV ou Excel files but SQL databases, you can directly import them to Big Query without using Google Cloud Storage. To do so you will need to find the following information: \n",
    "\n",
    "* **Cloud SQL Instance ID**\n",
    "\n",
    "![crack](https://essentials-assets.s3.eu-west-3.amazonaws.com/M02-SQL/Introduction_to_SQL_and_cloud_computing/cloud_sql_instance_id.png)\n",
    "\n",
    "* **Database name**\n",
    "\n",
    "![crack](https://essentials-assets.s3.eu-west-3.amazonaws.com/M02-SQL/Introduction_to_SQL_and_cloud_computing/cloud_sql_database.png)\n",
    "    \n",
    "* **Database username** \n",
    "\n",
    "![crack](https://essentials-assets.s3.eu-west-3.amazonaws.com/M02-SQL/Introduction_to_SQL_and_cloud_computing/cloud_sql_users.png)\n",
    "\n",
    "* Database password \n",
    "    * You should have stored it when you created your db \n",
    "\n",
    "\n",
    "Once you have gathered this information, go to Google Big Query in the *SQL Workspace* section and above click on *+ ADD DATA > External Data Source*\n",
    "\n",
    "![crack](https://essentials-assets.s3.eu-west-3.amazonaws.com/M02-SQL/Introduction_to_SQL_and_cloud_computing/Cloud_sql_add_external_data_source.png)\n",
    "\n",
    "Then simply fill out the information you need. Additionnally to what is above, you will need to provide a *Connection ID*. It is you ðŸ‘Š who creates it, you don't need to find it somewhere in GCP ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources ðŸ“šðŸ“š\n",
    "\n",
    "* Google Big Query - [https://bit.ly/ckdaXc](https://cloud.google.com/bigquery)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
